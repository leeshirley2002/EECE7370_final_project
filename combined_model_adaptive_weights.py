# -*- coding: utf-8 -*-
"""Combined_Model_Adaptive_Weights.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QvPMIbwtqIXc2sr51b3hDDwRxXNQcSJx

# Combined Model with Adaptive Weights

This notebook combines four trained models using adaptive weights for final prediction.

## Models:
- **Model A (Eye)**: Eye region detection (checkpoint: `Model_eyes_nose/model_a_eyes_best.pth`)
- **Model A (Nose)**: Nose region detection (checkpoint: `Model_eyes_nose/model_a_nose_best.pth`)
- **Model B (Mouth)**: Mouth region detection (checkpoint: `Model_month/CNN_Mouth_best_1207_0909.pth`)
- **Model C (Face)**: Full face detection (checkpoint: `Model_face/paper_model_c_best.pth`)

## Adaptive Weight Method:
- Learns optimal weights for combining predictions from four models
- Weights are learned on validation set
- Final prediction = weighted average of four model predictions

## Install Required Packages
"""

# Commented out IPython magic to ensure Python compatibility.
# Install required packages (if not already installed)
# %pip install torch torchvision tqdm scikit-learn pillow numpy -q

# Check GPU availability
import torch
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")

"""## Import Libraries

"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
from pathlib import Path
from tqdm import tqdm
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import json
from typing import Tuple, Dict, List

"""## Model Architecture Definitions

**Why do we need to define the architectures here?**

PyTorch checkpoints (`.pth` files) only store the **weights** (state_dict), not the model architecture itself. To load a trained model, we need to:

1. **First**: Define/create the model architecture (this cell)
2. **Then**: Load the weights from the checkpoint into that architecture

Think of it like this:
- **Checkpoint** = The trained weights (the "brain")
- **Architecture definition** = The model structure (the "skeleton")
- We need both: the skeleton to hold the brain!

**Models defined here:**
- **ModelA**: 12-layer CNN with BatchNorm (for Eye and Nose regions, input: 50x50)
- **CNN_ModelA**: CNN architecture (for Mouth region, input: 64x64)
- **PaperModelC**: CNN-ViT hybrid (for Full Face region, input: 224x224)

"""

# ============================================================================
# MODEL ARCHITECTURES
# ============================================================================
# Model A: 12-layer CNN with BatchNorm (for Eye and Nose, input: 50x50)
# Model B: CNN_ModelA (for Mouth, input: 64x64)
# Model C: CNN-ViT Hybrid (for Full Face, input: 224x224)
# ============================================================================

import cv2
import numpy as np

# Standard parameters to map [0, 1] data to [-1, 1]
CUSTOM_MEAN = np.array([0.5, 0.5, 0.5], dtype=np.float32)
CUSTOM_STD = np.array([0.5, 0.5, 0.5], dtype=np.float32)

# Custom Transform for Model A (Eye/Nose)
# Replicates: BGR -> RGB -> [0, 1] scale -> [-1, 1] scale -> HWC to CHW Tensor
class ModelATransform:
    def __call__(self, img_np):
        # img_np is guaranteed to be a NumPy array here, even if zero-filled

        # 1. BGR to RGB Conversion (Crucial for OpenCV-trained Model A)
        # This assumes the input img_np is a raw NumPy array in BGR order.
        img_np = cv2.cvtColor(img_np, cv2.COLOR_BGR2RGB)

        # 2. Scale to [0, 1] (Exact replication of training)
        img_np = img_np.astype(np.float32) / 255.0

        # 3. Apply [-1, 1] normalization
        img_np = (img_np - CUSTOM_MEAN) / CUSTOM_STD

        # 4. Convert to Tensor (CHW)
        img_tensor = torch.FloatTensor(img_np).permute(2, 0, 1)
        return img_tensor

# Custom Transform for Model B (Mouth)
# Replicates: [0, 1] scale -> [-1, 1] scale -> HWC to CHW Tensor (Model B used PIL, which is RGB)
class ModelBTransform:
    def __call__(self, img_np):
        # img_np is guaranteed to be a NumPy array here, even if zero-filled

        # 1. Scale to [0, 1] (Exact replication of training)
        img_np = img_np.astype(np.float32) / 255.0

        # 2. Apply [-1, 1] normalization
        img_np = (img_np - CUSTOM_MEAN) / CUSTOM_STD

        # 3. Convert to Tensor (CHW)
        img_tensor = torch.FloatTensor(img_np).permute(2, 0, 1)
        return img_tensor


# Model A: 12-layer CNN with BatchNorm (from model_ab_paper.py)
# ============================================================================

class ModelA(nn.Module):
    """
    Paper Model A: 12 layers with Batch Normalization
    Input: 50x50x3
    Three blocks: Conv -> BatchNorm -> ReLU -> MaxPool -> Dropout
    Used for Eye and Nose regions
    """
    def __init__(self):
        super(ModelA, self).__init__()

        # Block 1
        self.block1 = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.Conv2d(32, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
            nn.Dropout(0.3)
        )

        # Block 2
        self.block2 = nn.Sequential(
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
            nn.Dropout(0.3)
        )

        # Block 3
        self.block3 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.Conv2d(128, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
            nn.Dropout(0.3)
        )

        # Classifier
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(128 * 6 * 6, 512),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(512, 2)  # 2 classes: Real, Fake
        )

    def forward(self, x):
        x = self.block1(x)
        x = self.block2(x)
        x = self.block3(x)
        x = self.classifier(x)
        return x


# ============================================================================
# MODEL B: CNN_ModelA (from CNN_Deepfacke_Detection_Model_mouth.ipynb)
# ============================================================================

class CNN_ModelA(nn.Module):
    """
    CNN Model A for Mouth region
    Input: 64x64x3
    Used for Mouth region detection
    """
    def __init__(self, num_classes=2):
        super(CNN_ModelA, self).__init__()

        # Convolutional Blocks
        self.features = nn.Sequential(
            # ---- Block 1 ----
            nn.Conv2d(3, 32, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(32),

            nn.Conv2d(32, 32, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(32),

            nn.Conv2d(32, 32, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(32),

            nn.MaxPool2d(kernel_size=2, stride=2),  # 64x64 -> 32x32
            nn.Dropout(0.3),

            # ---- Block 2 ----
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(64),

            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(64),

            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(64),

            nn.MaxPool2d(kernel_size=2, stride=2),  # 32x32 -> 16x16
            nn.Dropout(0.3),

            # ---- Block 3 ----
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(128),

            nn.Conv2d(128, 128, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(128),

            nn.Conv2d(128, 128, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(128),

            nn.MaxPool2d(kernel_size=2, stride=2),  # 16x16 -> 8x8
            nn.Dropout(0.3),
        )

        # Classifier
        # Input 64x64; after 3 pools: 128*8*8
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(128 * 8 * 8, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(512, num_classes)  # CrossEntropyLoss, no softmax
        )

    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x


# ============================================================================
# MODEL C: CNN-ViT Hybrid (PaperModelC)
# ============================================================================

class MultiHeadSelfAttention(nn.Module):
    """Multi-Head Self-Attention mechanism"""
    def __init__(self, embed_dim: int = 1024, num_heads: int = 8, dropout: float = 0.1):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads

        assert self.head_dim * num_heads == embed_dim, "embed_dim must be divisible by num_heads"

        self.qkv = nn.Linear(embed_dim, embed_dim * 3)
        self.proj = nn.Linear(embed_dim, embed_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B, N, C = x.shape

        # Generate Q, K, V
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]

        # Scaled dot-product attention
        attn = (q @ k.transpose(-2, -1)) * (self.head_dim ** -0.5)
        attn = attn.softmax(dim=-1)
        attn = self.dropout(attn)

        # Apply attention to values
        x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        x = self.dropout(x)

        return x


class TransformerBlock(nn.Module):
    """Transformer encoder block"""
    def __init__(self, dim: int, heads: int, mlp_dim: int, dropout: float = 0.1):
        super().__init__()
        self.norm1 = nn.LayerNorm(dim)
        self.attn = MultiHeadSelfAttention(dim, heads, dropout)
        self.norm2 = nn.LayerNorm(dim)

        self.mlp = nn.Sequential(
            nn.Linear(dim, mlp_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(mlp_dim, dim),
            nn.Dropout(dropout)
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Pre-norm architecture
        x = x + self.attn(self.norm1(x))
        x = x + self.mlp(self.norm2(x))
        return x


class SimpleCNNModule(nn.Module):
    """Simple CNN Module as per paper's Model C"""
    def __init__(self, in_channels=3, out_channels=32):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        self.out_channels = out_channels

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.features(x)


class PaperViTModule(nn.Module):
    """Vision Transformer Module - processes CNN feature maps"""
    def __init__(self, feature_size: int, patch_size: int = 7,
                 dim: int = 1024, depth: int = 6, heads: int = 8,
                 mlp_dim: int = 2048, dropout: float = 0.1):
        super().__init__()
        self.feature_size = feature_size
        self.patch_size = patch_size
        self.dim = dim
        self.depth = depth

        # Calculate number of patches
        self.n_patches = (feature_size // patch_size) ** 2

        # Patch embedding: convert patches to vectors
        self.patch_embed = nn.Conv2d(
            32, dim, kernel_size=patch_size, stride=patch_size
        )

        # Position embeddings
        self.pos_embed = nn.Parameter(torch.zeros(1, self.n_patches, dim))

        # Transformer encoder blocks
        self.blocks = nn.ModuleList([
            TransformerBlock(dim, heads, mlp_dim, dropout)
            for _ in range(depth)
        ])

        self.norm = nn.LayerNorm(dim)
        self.dropout = nn.Dropout(dropout)

        # Initialize weights
        nn.init.trunc_normal_(self.pos_embed, std=0.02)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B = x.shape[0]

        # Patch embedding: (B, 32, H, W) -> (B, dim, H', W')
        x = self.patch_embed(x)

        # Flatten spatial dimensions: (B, dim, H', W') -> (B, n_patches, dim)
        B, C, H, W = x.shape
        x = x.flatten(2).transpose(1, 2)  # (B, n_patches, dim)

        # Add position embedding
        x = x + self.pos_embed
        x = self.dropout(x)

        # Apply transformer blocks
        for block in self.blocks:
            x = block(x)

        x = self.norm(x)

        return x  # (B, n_patches, dim)


class PaperModelC(nn.Module):
    """Model C from Paper: CNN-ViT for Full Face Detection"""
    def __init__(
        self,
        img_size: int = 224,
        num_classes: int = 2,
        patch_size: int = 7,
        dim: int = 1024,
        depth: int = 6,
        heads: int = 8,
        mlp_dim: int = 2048,
        dropout: float = 0.1
    ):
        super().__init__()

        self.img_size = img_size

        # CNN Module
        self.cnn_module = SimpleCNNModule(in_channels=3, out_channels=32)

        # Calculate feature size after CNN (224 -> 112 after MaxPool)
        feature_size = img_size // 2

        # ViT Module
        self.vit_module = PaperViTModule(
            feature_size=feature_size,
            patch_size=patch_size,
            dim=dim,
            depth=depth,
            heads=heads,
            mlp_dim=mlp_dim,
            dropout=dropout
        )

        # Global Average Pooling
        self.global_pool = nn.AdaptiveAvgPool1d(1)

        # Classification Head
        self.classifier = nn.Linear(dim, num_classes)

        # Initialize weights
        self._initialize_weights()

    def _initialize_weights(self):
        """Initialize weights"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.trunc_normal_(m.weight, std=0.02)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # CNN feature extraction: (B, 3, 224, 224) -> (B, 32, 112, 112)
        cnn_features = self.cnn_module(x)

        # ViT processing: (B, 32, 112, 112) -> (B, n_patches, dim)
        vit_features = self.vit_module(cnn_features)  # (B, n_patches, dim)

        # Global Average Pooling: (B, n_patches, dim) -> (B, dim)
        vit_features = vit_features.transpose(1, 2)  # (B, dim, n_patches)
        pooled = self.global_pool(vit_features).squeeze(-1)  # (B, dim)

        # Classification: (B, dim) -> (B, num_classes)d
        logits = self.classifier(pooled)

        return logits

print("✓ All model architectures defined successfully!")
print("  - ModelA (Eye/Nose): 50x50 input")
print("  - CNN_ModelA (Mouth): 64x64 input")
print("  - PaperModelC (Face): 224x224 input")

"""## Combined Model with Adaptive Weights

The combined model uses learnable weights to combine predictions from four models (Eye, Nose, Mouth, Face).

"""

class CombinedModelAdaptiveWeights(nn.Module):
    """
    Combined model that uses adaptive weights to fuse predictions from four models.

    Architecture:
    - Model A (Eye): Processes eye region images (50x50)
    - Model A (Nose): Processes nose region images (50x50)
    - Model B (Mouth): Processes mouth region images (64x64)
    - Model C (Face): Processes full face images (224x224)
    - Adaptive Weight Layer: Learns optimal weights for combining predictions
    """
    def __init__(
        self,
        model_eye: nn.Module,
        model_nose: nn.Module,
        model_mouth: nn.Module,
        model_face: nn.Module,
        learn_weights: bool = True
    ):
        super().__init__()

        # Freeze individual models (only train weights)
        self.model_eye = model_eye
        self.model_nose = model_nose
        self.model_mouth = model_mouth
        self.model_face = model_face

        # Freeze all parameters in individual models
        for param in self.model_eye.parameters():
            param.requires_grad = False
        for param in self.model_nose.parameters():
            param.requires_grad = False
        for param in self.model_mouth.parameters():
            param.requires_grad = False
        for param in self.model_face.parameters():
            param.requires_grad = False

        # Adaptive weights (learnable parameters)
        if learn_weights:
            # Method 1: Simple learnable weights (sum to 1 via softmax)
            self.weight_logits = nn.Parameter(torch.ones(4))  # [w_eye, w_nose, w_mouth, w_face]
        else:
            # Method 2: Fixed equal weights
            self.register_buffer('weight_logits', torch.ones(4) / 4)

        self.learn_weights = learn_weights

    def get_weights(self) -> torch.Tensor:
        """Get normalized weights (sum to 1)"""
        if self.learn_weights:
            # Apply softmax to ensure weights sum to 1
            weights = F.softmax(self.weight_logits, dim=0)
        else:
            weights = self.weight_logits
        return weights

    def forward(
        self,
        eye_images: torch.Tensor = None,
        nose_images: torch.Tensor = None,
        mouth_images: torch.Tensor = None,
        face_images: torch.Tensor = None
    ) -> torch.Tensor:
        """
        Forward pass with adaptive weight fusion.

        Args:
            eye_images: (B, 3, 50, 50) - Eye region images for Model A
            nose_images: (B, 3, 50, 50) - Nose region images for Model A
            mouth_images: (B, 3, 64, 64) - Mouth region images for Model B
            face_images: (B, 3, 224, 224) - Full face images for Model C

        Returns:
            Combined logits: (B, 2) - Final prediction logits
        """
        # Get predictions from each model and track which models are active
        predictions = []
        active_indices = []  # Track which weight indices to use

        if eye_images is not None:
            pred_eye = self.model_eye(eye_images)  # (B, 2)
            predictions.append(pred_eye)
            active_indices.append(0)  # Eye is weight index 0

        if nose_images is not None:
            pred_nose = self.model_nose(nose_images)  # (B, 2)
            predictions.append(pred_nose)
            active_indices.append(1)  # Nose is weight index 1

        if mouth_images is not None:
            pred_mouth = self.model_mouth(mouth_images)  # (B, 2)
            predictions.append(pred_mouth)
            active_indices.append(2)  # Mouth is weight index 2

        if face_images is not None:
            pred_face = self.model_face(face_images)  # (B, 2)
            predictions.append(pred_face)
            active_indices.append(3)  # Face is weight index 3

        if len(predictions) == 0:
            raise ValueError("At least one input image must be provided")

        # Stack predictions: (num_active_models, B, 2)
        pred_stack = torch.stack(predictions, dim=0)  # (num_active_models, B, 2)
        pred_probs = F.softmax(pred_stack, dim=-1)  # (num_active_models, B, 2) - all on [0,1]
        pred_stack = torch.log(pred_probs + 1e-8)

        # Get adaptive weights and select only active ones
        all_weights = self.get_weights()  # (4,)
        active_weights = all_weights[active_indices]  # (num_active_models,)

        # Normalize active weights to sum to 1
        active_weights = active_weights / active_weights.sum()

        # Weighted combination: (num_active_models, B, 2) * (num_active_models, 1, 1) -> (B, 2)
        active_weights = active_weights.view(-1, 1, 1)  # (num_active_models, 1, 1)
        combined_logits = (pred_stack * active_weights).sum(dim=0)  # (B, 2)

        return combined_logits

    def predict_single(
        self,
        eye_image: torch.Tensor = None,
        nose_image: torch.Tensor = None,
        mouth_image: torch.Tensor = None,
        face_image: torch.Tensor = None
    ) -> Dict[str, float]:
        """
        Predict single image(s) and return detailed results.

        Returns:
            Dictionary with prediction, confidence, probabilities, and weights
        """
        self.eval()
        with torch.no_grad():
            logits = self.forward(eye_image, nose_image, mouth_image, face_image)
            probs = F.softmax(logits, dim=1)
            pred = torch.argmax(logits, dim=1)

            weights = self.get_weights().detach().cpu().numpy()

            return {
                'prediction': 'Real' if pred.item() == 0 else 'Fake',
                'confidence': probs[0][pred.item()].item(),
                'real_prob': probs[0][0].item(),
                'fake_prob': probs[0][1].item(),
                'weights': {
                    'eye_model': float(weights[0]),
                    'nose_model': float(weights[1]),
                    'mouth_model': float(weights[2]),
                    'face_model': float(weights[3])
                }
            }

print("✓ Combined model with adaptive weights defined!")

"""## Load Trained Models

Load the four pre-trained models from checkpoints.

"""

from google.colab import drive
drive.mount('/content/drive')
# Configuration
config = {
    'model_eye_path': '/content/drive/MyDrive/Model_eyes_nose/model_a_eyes_best.pth',  # Eye region model (ModelA)
    'model_nose_path': '/content/drive/MyDrive/Model_eyes_nose/model_a_nose_best.pth',  # Nose region model (ModelA)
    'model_mouth_path': '/content/drive/MyDrive/Model_mouth/CNN_Mouth_best_1207_0909.pth',  # Mouth region model (CNN_ModelA)
    'model_face_path': '/content/drive/MyDrive/Model_face/paper_model_c_best.pth',  # Full face model (PaperModelC)
    'device': 'cuda' if torch.cuda.is_available() else 'cpu',
    'img_size': 224,  # For Model C (Face)
    'patch_size': 7,
    'dim': 1024,
    'depth': 6,
    'heads': 8,
    'mlp_dim': 2048,
}

print(f"Device: {config['device']}")
print(f"Loading models from checkpoints...")

def load_model(checkpoint_path: str, model_name: str, model_type: str, device: str) -> nn.Module:
    """
    Load a trained model from checkpoint.

    Process:
    1. Create model architecture (empty model with random weights)
    2. Load trained weights from checkpoint into the architecture
    3. Return the model with trained weights loaded

    Args:
        checkpoint_path: Path to checkpoint file
        model_name: Name for display
        model_type: 'ModelA', 'CNN_ModelA', or 'PaperModelC'
        device: Device to load model on
    """
    print(f"\nLoading {model_name} from {checkpoint_path}...")

    # Step 1: Create model architecture based on type
    if model_type == 'ModelA':
        model = ModelA()
    elif model_type == 'CNN_ModelA':
        model = CNN_ModelA(num_classes=2)
    elif model_type == 'PaperModelC':
        model = PaperModelC(
            img_size=config['img_size'],
            num_classes=2,
            patch_size=config['patch_size'],
            dim=config['dim'],
            depth=config['depth'],
            heads=config['heads'],
            mlp_dim=config['mlp_dim'],
            dropout=0.1
        )
    else:
        raise ValueError(f"Unknown model_type: {model_type}")

    # Step 2: Load the trained weights from checkpoint
    checkpoint = torch.load(checkpoint_path, map_location=device)

    # Handle different checkpoint formats
    if isinstance(checkpoint, dict):
        if 'model_state_dict' in checkpoint:
            # Checkpoint contains model_state_dict, optimizer, etc.
            model.load_state_dict(checkpoint['model_state_dict'])
            if 'val_acc' in checkpoint:
                print(f"  Validation accuracy: {checkpoint['val_acc']*100:.2f}%")
        else:
            # Checkpoint is the state dict directly
            model.load_state_dict(checkpoint)
    else:
        # Checkpoint is just the state dict
        model.load_state_dict(checkpoint)

    # Step 3: Move to device and set to evaluation mode
    model = model.to(device)
    model.eval()  # Set to evaluation mode (disables dropout, etc.)

    print(f"  ✓ {model_name} loaded successfully with trained weights!")

    return model

# Load all four models
model_eye = load_model(config['model_eye_path'], 'Model A (Eye)', 'ModelA', config['device'])
model_nose = load_model(config['model_nose_path'], 'Model A (Nose)', 'ModelA', config['device'])
model_mouth = load_model(config['model_mouth_path'], 'Model B (Mouth)', 'CNN_ModelA', config['device'])
model_face = load_model(config['model_face_path'], 'Model C (Face)', 'PaperModelC', config['device'])

print("\n✓ All models loaded successfully!")

"""## Create Combined Model

Create the combined model with adaptive weights.

"""

# Create combined model with adaptive weights
combined_model = CombinedModelAdaptiveWeights(
    model_eye=model_eye,
    model_nose=model_nose,
    model_mouth=model_mouth,
    model_face=model_face,
    learn_weights=True  # Enable learning of adaptive weights
)

combined_model = combined_model.to(config['device'])

# Print initial weights
initial_weights = combined_model.get_weights().detach().cpu().numpy()
print("\nInitial Adaptive Weights:")
print(f"  Model A (Eye):   {initial_weights[0]:.4f} ({initial_weights[0]*100:.2f}%)")
print(f"  Model A (Nose):  {initial_weights[1]:.4f} ({initial_weights[1]*100:.2f}%)")
print(f"  Model B (Mouth): {initial_weights[2]:.4f} ({initial_weights[2]*100:.2f}%)")
print(f"  Model C (Face):  {initial_weights[3]:.4f} ({initial_weights[3]*100:.2f}%)")
print(f"  Sum: {initial_weights.sum():.4f}")

# Count parameters
total_params = sum(p.numel() for p in combined_model.parameters())
trainable_params = sum(p.numel() for p in combined_model.parameters() if p.requires_grad)
print(f"\nCombined Model Parameters:")
print(f"  Total parameters: {total_params:,}")
print(f"  Trainable parameters (weights only): {trainable_params:,}")
print(f"  Frozen parameters: {total_params - trainable_params:,}")

# ============================================================================
# Training Configuration
# ============================================================================

train_config = {
    'batch_size': 16,
    'num_epochs': 20,
    'learning_rate': 0.01,  # Higher LR for just 4 weight parameters
    'val_split': 0.2,  # 20% for validation
    'num_workers': 8, # Temporarily set to 0 for debugging DataLoader errors
    'device': config['device'],
    'dataset_cache_path': '/content/drive/MyDrive/dataset_cache.json', # Path to cache dataset info
}

print("Training Configuration:")
for key, value in train_config.items():
    print(f"  {key}: {value}")

import torch # ensure torch is imported
import cv2 # Import OpenCV
import numpy as np # Import numpy

# ============================================================================
# Create Dataset with Proper Transforms for Each Region
# ============================================================================

from torch.utils.data import random_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Custom Transform for Model A (Eye/Nose)
# Replicates: BGR -> RGB -> [0, 1] scale -> [-1, 1] scale -> HWC to CHW Tensor
class ModelATransform:
    def __call__(self, img_np):
        # img_np is guaranteed to be a NumPy array here, even if zero-filled

        # 1. BGR to RGB Conversion (Crucial for OpenCV-trained Model A)
        # This assumes the input img_np is a raw NumPy array in BGR order.
        img_np = cv2.cvtColor(img_np, cv2.COLOR_BGR2RGB)

        # 2. Scale to [0, 1] (Exact replication of training)
        img_np = img_np.astype(np.float32) / 255.0

        # 3. Apply [-1, 1] normalization
        img_np = (img_np - CUSTOM_MEAN) / CUSTOM_STD

        # 4. Convert to Tensor (CHW)
        img_tensor = torch.FloatTensor(img_np).permute(2, 0, 1)
        return img_tensor

# Custom Transform for Model B (Mouth)
# Replicates: [0, 1] scale -> [-1, 1] scale -> HWC to CHW Tensor (Model B used PIL, which is RGB)
class ModelBTransform:
    def __call__(self, img_np):
        # img_np is guaranteed to be a NumPy array here, even if zero-filled

        # 1. Scale to [0, 1] (Exact replication of training)
        img_np = img_np.astype(np.float32) / 255.0

        # 2. Apply [-1, 1] normalization
        img_np = (img_np - CUSTOM_MEAN) / CUSTOM_STD

        # 3. Convert to Tensor (CHW)
        img_tensor = torch.FloatTensor(img_np).permute(2, 0, 1)
        return img_tensor

# Custom Transform for Model A (Eye/Nose)
transform_eye_nose = ModelATransform()

# Custom Transform for Model B (Mouth)
transform_mouth = ModelBTransform()

# Face transform remains ImageNet normalization
transform_face = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Create transforms dict
transforms_dict = {
    'eye': transform_eye_nose,
    'nose': transform_eye_nose,
    'mouth': transform_mouth,
    'face': transform_face
}

# ============================================================================
# OPTIMIZATION: Pre-build lookup dictionary (one-time scan, much faster!)
# ============================================================================
print("\nBuilding feature file lookup dictionary (one-time scan)...")
from collections import defaultdict
import time

start_time = time.time()
features_dir = Path('/content/drive/MyDrive/Features')
feature_lookup = defaultdict(dict)  # {identifier: {'leftEye': path, 'rightEye': path, 'nose': path, 'mouth': path}}

# Scan all feature directories ONCE
for folder_type in ['original', 'manipulated']:
    for region in ['leftEye', 'rightEye', 'nose', 'mouth']:
        region_dir = features_dir / folder_type / region
        if region_dir.exists():
            all_files = list(region_dir.glob('*.jpg'))
            for file_path in all_files:
                # Extract identifier: frame{num}_face{num} from filename
                # Pattern: {type}_{type}_frame{num}_face{num}_face{num}_{region}.jpg
                filename = file_path.stem
                parts = filename.split('_')

                # Find frame and face parts
                identifier = None
                for i, part in enumerate(parts):
                    if part.startswith('frame') and i + 1 < len(parts):
                        frame_part = part
                        face_part = parts[i + 1] if parts[i + 1].startswith('face') else None
                        if face_part:
                            identifier = f"{frame_part}_{face_part}"
                            break

                if identifier and region not in feature_lookup[identifier]:
                    feature_lookup[identifier][region] = file_path

elapsed = time.time() - start_time
print(f"✓ Lookup dictionary built in {elapsed:.2f} seconds")
print(f"  Found {len(feature_lookup)} unique identifiers")

# Create optimized dataset class
class OptimizedMultiRegionDataset(Dataset):
    """Fast version using pre-built lookup dictionary"""
    def __init__(self, faces_dir, feature_lookup, transform=None, use_both_eyes=True, cache_path=None):
        self.faces_dir = Path(faces_dir)
        self.feature_lookup = feature_lookup
        self.transform = transform
        self.use_both_eyes = use_both_eyes
        self.cache_path = Path(cache_path) if cache_path else None

        self.image_info = []
        self.labels = []

        if self.cache_path and self.cache_path.exists():
            print(f"Loading dataset info from cache: {self.cache_path}")
            with open(self.cache_path, 'r') as f:
                cache_data = json.load(f)
            self.image_info = cache_data['image_info']
            # Convert string paths back to Path objects
            for info in self.image_info:
                for key in ['face_path', 'eye_left_path', 'eye_right_path', 'nose_path', 'mouth_path']:
                    if info[key]:
                        info[key] = Path(info[key])
            self.labels = cache_data['labels']
            print(f"  Loaded {len(self.image_info)} samples from cache.")
        else:
            print(f"Cache not found or not specified. Scanning file system...")
            original_faces = list((self.faces_dir / 'original').glob('*.jpg'))
            manipulated_faces = list((self.faces_dir / 'manipulated').glob('*.jpg'))

            self.face_images = original_faces + manipulated_faces
            self.labels = [0] * len(original_faces) + [1] * len(manipulated_faces)

            # Build image info using lookup (instant, no glob search!)
            for face_path in self.face_images:
                face_name = face_path.stem
                if face_name.startswith('original_'):
                    identifier = face_name.replace('original_', '')
                elif face_name.startswith('manipulated_'):
                    identifier = face_name.replace('manipulated_', '')
                else:
                    identifier = face_name

                features = feature_lookup.get(identifier, {})
                self.image_info.append({
                    'face_path': face_path,
                    'eye_left_path': features.get('leftEye'),
                    'eye_right_path': features.get('rightEye'),
                    'nose_path': features.get('nose'),
                    'mouth_path': features.get('mouth'),
                    'identifier': identifier
                })

            # Save to cache if cache_path is provided
            if self.cache_path:
                print(f"Saving dataset info to cache: {self.cache_path}")
                serializable_image_info = []
                for info in self.image_info:
                    serializable_info = {}
                    for key, value in info.items():
                        serializable_info[key] = str(value) if isinstance(value, Path) else value
                    serializable_image_info.append(serializable_info)

                cache_data = {
                    'image_info': serializable_image_info,
                    'labels': self.labels
                }
                # Create directory if it doesn't exist
                self.cache_path.parent.mkdir(parents=True, exist_ok=True)
                with open(self.cache_path, 'w') as f:
                    json.dump(cache_data, f, indent=4)
                print("  Cache saved successfully.")

        eye_count = sum(1 for info in self.image_info if info['eye_left_path'])
        nose_count = sum(1 for info in self.image_info if info['nose_path'])
        mouth_count = sum(1 for info in self.image_info if info['mouth_path'])
        print(f"\nDataset: {len(self.image_info)} faces, {eye_count} eyes, {nose_count} noses, {mouth_count} mouths")

    def __len__(self):
        return len(self.image_info)

    def _combine_eyes(self, left_eye, right_eye):
        target_size = (50, 50) # Target size for Model A (Eye/Nose)

        resized_left_eye = None
        if left_eye is not None:
            resized_left_eye = cv2.resize(left_eye, target_size, interpolation=cv2.INTER_AREA)

        resized_right_eye = None
        if right_eye is not None:
            resized_right_eye = cv2.resize(right_eye, target_size, interpolation=cv2.INTER_AREA)

        if resized_left_eye is None and resized_right_eye is None:
            # If both are None, return a zero-filled numpy array matching the expected output shape and dtype
            return np.zeros((target_size[1], target_size[0], 3), dtype=np.uint8)
        elif resized_left_eye is None:
            return resized_right_eye
        elif resized_right_eye is None:
            return resized_left_eye
        else:
            # Both are now resized to (50, 50, 3) uint8
            # Average pixel values, resulting in float32, then cast back to uint8
            combined_avg_float = (resized_left_eye.astype(np.float32) + resized_right_eye.astype(np.float32)) / 2
            # Clip to [0, 255] and convert back to uint8 for consistency
            return np.clip(combined_avg_float, 0, 255).astype(np.uint8)

    def __getitem__(self, idx):
        info = self.image_info[idx]
        label = self.labels[idx]

        # --- Load Face Image (PIL RGB) ---
        face_obj = None
        if info['face_path'] and Path(info['face_path']).exists():
            try:
                face_obj = Image.open(info['face_path']).convert('RGB')
            except (FileNotFoundError, OSError) as e:
                print(f"Warning: Could not load face image {info['face_path']}: {e}. Returning zeros for this sample.")

        # If face image cannot be loaded, or path is invalid, return dummy tensors for all
        if face_obj is None:
            return {
                'face': torch.zeros(3, 224, 224),
                'eye': torch.zeros(3, 50, 50),
                'nose': torch.zeros(3, 50, 50),
                'mouth': torch.zeros(3, 64, 64),
                'label': label,
                'identifier': info['identifier']
            }

        # --- Load Eye Images (NumPy BGR) ---
        eye_obj_np_processed = None
        left_eye_raw_np = None
        right_eye_raw_np = None

        if info['eye_left_path'] and Path(info['eye_left_path']).exists():
            try:
                left_eye_raw_np = cv2.imread(str(info['eye_left_path'])) # Load as BGR NumPy
            except Exception as e:
                print(f"Warning: Could not load left eye image {info['eye_left_path']}: {e}.")

        if info['eye_right_path'] and Path(info['eye_right_path']).exists():
            try:
                right_eye_raw_np = cv2.imread(str(info['eye_right_path'])) # Load as BGR NumPy
            except Exception as e:
                print(f"Warning: Could not load right eye image {info['eye_right_path']}: {e}.")

        # Pass raw NumPy arrays to _combine_eyes for resizing and combining/concatenating
        # _combine_eyes now ensures a (50,50,3) uint8 NumPy array output
        eye_obj_np_processed = self._combine_eyes(left_eye_raw_np, right_eye_raw_np)

        # --- Load Nose Image (NumPy BGR) ---
        nose_obj_np = np.zeros((50, 50, 3), dtype=np.uint8) # Default to zero array
        if info['nose_path'] and Path(info['nose_path']).exists():
            try:
                loaded_nose_np = cv2.imread(str(info['nose_path'])) # Load as BGR NumPy
                if loaded_nose_np is not None: # Ensure loaded_nose_np is not None
                    nose_obj_np = cv2.resize(loaded_nose_np, (50, 50), interpolation=cv2.INTER_AREA)
            except Exception as e:
                print(f"Warning: Could not load nose image {info['nose_path']}: {e}.")

        # --- Load Mouth Image (NumPy RGB) ---
        mouth_obj_np = np.zeros((64, 64, 3), dtype=np.uint8) # Default to zero array
        if info['mouth_path'] and Path(info['mouth_path']).exists():
            try:
                mouth_pil = Image.open(info['mouth_path']).convert('RGB')
                if mouth_pil is not None: # Ensure PIL.Image.open didn't return None on error
                    loaded_mouth_np = np.array(mouth_pil) # Convert PIL RGB to NumPy RGB
                    mouth_obj_np = cv2.resize(loaded_mouth_np, (64, 64), interpolation=cv2.INTER_AREA)
            except (FileNotFoundError, OSError, Exception) as e:
                print(f"Warning: Could not load mouth image {info['mouth_path']}: {e}.")

        # --- Apply Transforms ---
        transformed_face = self.transform['face'](face_obj)
        # Pass NumPy arrays (guaranteed not None and resized) to transforms
        transformed_eye = self.transform['eye'](eye_obj_np_processed)
        transformed_nose = self.transform['nose'](nose_obj_np)
        transformed_mouth = self.transform['mouth'](mouth_obj_np)

        return {
            'face': transformed_face,
            'eye': transformed_eye,
            'nose': transformed_nose,
            'mouth': transformed_mouth,
            'label': label,
            'identifier': info['identifier']
        }

# Create optimized dataset (much faster!)
print("\nCreating optimized dataset...")
start_time = time.time()
full_dataset = OptimizedMultiRegionDataset(
    faces_dir='/content/drive/MyDrive/CroppedFaces',
    feature_lookup=feature_lookup,
    transform=transforms_dict,
    use_both_eyes=True,
    cache_path=train_config['dataset_cache_path']
)
elapsed = time.time() - start_time
print(f"✓ Dataset created in {elapsed:.2f} seconds")

# Split into train and validation
total_size = len(full_dataset)
val_size = int(total_size * train_config['val_split'])
train_size = total_size - val_size

train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size],
                                          generator=torch.Generator().manual_seed(42))

print(f"\nDataset Split:")
print(f"  Total samples: {total_size}")
print(f"  Training samples: {len(train_dataset)}")
print(f"  Validation samples: {len(val_dataset)}")

# Create data loaders
train_loader = DataLoader(
    train_dataset,
    batch_size=train_config['batch_size'],
    shuffle=True,
    num_workers=train_config['num_workers'],
    pin_memory=True if train_config['device'] == 'cuda' else False
)

val_loader = DataLoader(
    val_dataset,
    batch_size=train_config['batch_size'],
    shuffle=False,
    num_workers=train_config['num_workers'],
    pin_memory=True if train_config['device'] == 'cuda' else False
)

print(f"\nData loaders created!")
print(f"  Train batches: {len(train_loader)}")
print(f"  Val batches: {len(val_loader)}")

"""## Train Adaptive Weights

Train the adaptive weights on a validation set to optimize the combination of the four models.

"""

# ============================================================================
# Training Functions
# ============================================================================

def train_epoch(model, dataloader, criterion, optimizer, device):
    """Train for one epoch"""
    model.train()
    running_loss = 0.0
    all_preds = []
    all_labels = []

    # Track individual model logit scales for debugging
    logit_scales = {'eye': [], 'nose': [], 'mouth': [], 'face': []}

    for batch in tqdm(dataloader, desc="Training"):
        # Get inputs (handle None values)
        eye_imgs = batch['eye'].to(device) if batch['eye'] is not None else None
        nose_imgs = batch['nose'].to(device) if batch['nose'] is not None else None
        mouth_imgs = batch['mouth'].to(device) if batch['mouth'] is not None else None
        face_imgs = batch['face'].to(device)
        labels = batch['label'].to(device)

        # Debug: Check individual model logit scales (first batch only)
        if len(logit_scales['face']) == 0:
            with torch.no_grad():
                if eye_imgs is not None:
                    eye_logits = model.model_eye(eye_imgs)
                    logit_scales['eye'].append(eye_logits.abs().mean().item())
                if nose_imgs is not None:
                    nose_logits = model.model_nose(nose_imgs)
                    logit_scales['nose'].append(nose_logits.abs().mean().item())
                if mouth_imgs is not None:
                    mouth_logits = model.model_mouth(mouth_imgs)
                    logit_scales['mouth'].append(mouth_logits.abs().mean().item())
                if face_imgs is not None:
                    face_logits = model.model_face(face_imgs)
                    logit_scales['face'].append(face_logits.abs().mean().item())

        # Forward pass
        optimizer.zero_grad()
        logits = model(eye_imgs, nose_imgs, mouth_imgs, face_imgs)
        loss = criterion(logits, labels)

        # Backward pass
        loss.backward()

        # Gradient clipping for weight_logits to prevent extreme values
        torch.nn.utils.clip_grad_norm_([model.weight_logits], max_norm=0.5)  # Stricter clipping

        optimizer.step()

        # Constrain weights to prevent extreme values (soft constraint via regularization)
        # The weight_decay in optimizer should handle this, but we can add explicit constraint
        with torch.no_grad():
            # Ensure weights don't become too extreme (soft constraint)
            # This is handled by softmax, but we can add additional regularization
            pass

        # Statistics
        running_loss += loss.item()
        preds = torch.argmax(logits, dim=1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

    avg_loss = running_loss / len(dataloader)
    accuracy = accuracy_score(all_labels, all_preds)

    # Print logit scales if available
    if logit_scales['face']:
        print(f"\n  Logit scales (mean abs): Eye={logit_scales['eye'][0]:.2f}, "
              f"Nose={logit_scales['nose'][0]:.2f}, Mouth={logit_scales['mouth'][0]:.2f}, "
              f"Face={logit_scales['face'][0]:.2f}")

    return avg_loss, accuracy


def validate(model, dataloader, criterion, device):
    """Validate the model"""
    model.eval()
    running_loss = 0.0
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Validating"):
            # Get inputs
            eye_imgs = batch['eye'].to(device) if batch['eye'] is not None else None
            nose_imgs = batch['nose'].to(device) if batch['nose'] is not None else None
            mouth_imgs = batch['mouth'].to(device) if batch['mouth'] is not None else None
            face_imgs = batch['face'].to(device)
            labels = batch['label'].to(device)

            # Forward pass
            logits = model(eye_imgs, nose_imgs, mouth_imgs, face_imgs)
            loss = criterion(logits, labels)

            # Statistics
            running_loss += loss.item()
            preds = torch.argmax(logits, dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    avg_loss = running_loss / len(dataloader)
    accuracy = accuracy_score(all_labels, all_preds)
    precision = precision_score(all_labels, all_preds, average='binary', zero_division=0)
    recall = recall_score(all_labels, all_preds, average='binary', zero_division=0)
    f1 = f1_score(all_labels, all_preds, average='binary', zero_division=0)

    return avg_loss, accuracy, precision, recall, f1

print("✓ Training functions defined!")

# ============================================================================
# Setup Training
# ============================================================================

# Loss function
criterion = nn.CrossEntropyLoss()

# Optimizer - only optimize the weight parameters
# Use lower learning rate to prevent extreme weights
optimizer = optim.Adam(
    [combined_model.weight_logits],  # Only train the 4 weight parameters
    lr=train_config['learning_rate'] * 0.1  # Reduce LR to prevent extreme weights (0.001 instead of 0.01)
)

print(f"  Learning rate: {train_config['learning_rate'] * 0.1} (reduced to prevent extreme weights)")

# Learning rate scheduler
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='max', factor=0.5, patience=3
)

print("Training setup complete!")
print(f"  Optimizer: Adam (only optimizing 4 weight parameters)")
print(f"  Learning rate: {train_config['learning_rate']}")
print(f"  Loss function: CrossEntropyLoss")

# ============================================================================
# Training Loop
# ============================================================================

print("\n" + "="*60)
print("Starting Training...")
print("="*60)

# Track best validation accuracy
best_val_acc = 0.0
best_weights = None
history = {
    'train_loss': [],
    'train_acc': [],
    'val_loss': [],
    'val_acc': [],
    'val_precision': [],
    'val_recall': [],
    'val_f1': []
}

for epoch in range(1, train_config['num_epochs'] + 1):
    print(f"\nEpoch {epoch}/{train_config['num_epochs']}")
    print("-" * 60)

    # Train
    train_loss, train_acc = train_epoch(
        combined_model, train_loader, criterion, optimizer, train_config['device']
    )

    # Validate
    val_loss, val_acc, val_precision, val_recall, val_f1 = validate(
        combined_model, val_loader, criterion, train_config['device']
    )

    # Update learning rate
    scheduler.step(val_acc)

    # Save history
    history['train_loss'].append(train_loss)
    history['train_acc'].append(train_acc)
    history['val_loss'].append(val_loss)
    history['val_acc'].append(val_acc)
    history['val_precision'].append(val_precision)
    history['val_recall'].append(val_recall)
    history['val_f1'].append(val_f1)

    # Print metrics
    print(f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%")
    print(f"Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc*100:.2f}%")
    print(f"Val Precision: {val_precision*100:.2f}% | Recall: {val_recall*100:.2f}% | F1: {val_f1*100:.2f}%\n")

    # Save best weights
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        best_weights = combined_model.weight_logits.detach().clone()
        print(f"✓ New best validation accuracy: {best_val_acc*100:.2f}%")

    # Print current weights
    current_weights = combined_model.get_weights().detach().cpu().numpy()
    print(f"Current Weights: Eye={current_weights[0]:.3f}, Nose={current_weights[1]:.3f}, "
          f"Mouth={current_weights[2]:.3f}, Face={current_weights[3]:.3f}")

print("\n" + "="*60)
print("Training Complete!")
print("="*60)
print(f"Best Validation Accuracy: {best_val_acc*100:.2f}%")

# Load best weights
if best_weights is not None:
    combined_model.weight_logits.data = best_weights
    print("✓ Loaded best weights")

# Print final weights
final_weights = combined_model.get_weights().detach().cpu().numpy()
print("\nFinal Adaptive Weights:")
print(f"  Model A (Eye):   {final_weights[0]:.4f} ({final_weights[0]*100:.2f}%)")
print(f"  Model A (Nose):  {final_weights[1]:.4f} ({final_weights[1]*100:.2f}%)")
print(f"  Model B (Mouth): {final_weights[2]:.4f} ({final_weights[2]*100:.2f}%)")
print(f"  Model C (Face):  {final_weights[3]:.4f} ({final_weights[3]*100:.2f}%)")